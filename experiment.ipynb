{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tranhoang/.pyenv/versions/3.11.6/envs/alpha-bot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  27,  133,  114,   12,  455,    3,    9, 6871,    5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I would like to order a pizza.\"\n",
    "sentence_encoded = tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(sentence_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.65k/4.65k [00:00<00:00, 8.97MB/s]\n",
      "Downloading data: 100%|██████████| 11.3M/11.3M [00:02<00:00, 4.29MB/s]\n",
      "Downloading data: 100%|██████████| 442k/442k [00:01<00:00, 434kB/s]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:01<00:00, 837kB/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]/home/tranhoang/.pyenv/versions/3.11.6/envs/alpha-bot/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating train split: 12460 examples [00:00, 116040.72 examples/s]\n",
      "Generating validation split: 0 examples [00:00, ? examples/s]/home/tranhoang/.pyenv/versions/3.11.6/envs/alpha-bot/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating validation split: 500 examples [00:00, 73814.79 examples/s]\n",
      "Generating test split: 0 examples [00:00, ? examples/s]/home/tranhoang/.pyenv/versions/3.11.6/envs/alpha-bot/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating test split: 1500 examples [00:00, 120224.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dialog_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dialog_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][0]['dialogue'])\n",
    "print(dataset['test'][0]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary without prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Example  1\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION\n",
      "Brian, thank you for coming to our party.\n",
      "-------------------------------------------------\n",
      "Example  2\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION\n",
      "#Person1#: Yeah.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(50))\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    \n",
    "    generated_summary = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs= inputs['input_ids'],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION\\n{generated_summary}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Example  1\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION\n",
      "The train is about to leave.\n",
      "-------------------------------------------------\n",
      "Example  2\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following conversation\n",
    "\n",
    "        {dialogue}\n",
    "\n",
    "        Summary: \n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    generated_summary = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION\\n{generated_summary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summary):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        prompt += f\"\"\"\n",
    "            Dialogue:\n",
    "            {dialogue}\n",
    "            What was going on?\n",
    "            {summary}\n",
    "        \"\"\"\n",
    "    dialogue = dataset['test'][example_index_to_summary]['dialogue']\n",
    "    prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "        {dialogue}\n",
    "        What was going on?\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
