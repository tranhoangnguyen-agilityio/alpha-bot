{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    FunctionMessage,\n",
    "    ToolMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Variant\n",
    "\n",
    "All the chat messages have a streaming variant that contains `Chunk` in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessageChunk,\n",
    "    SystemMessageChunk,\n",
    "    HumanMessageChunk,\n",
    "    FunctionMessageChunk,\n",
    "    ToolMessageChunk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These chunks are used when streaming output from chat models, and they all define an additive property!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")\n",
    "AIMessageChunk(content='Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s implement a chat model that echoes back the first n characetrs of the last message in the prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s implement a chat model that echoes back the first n characetrs of the last message in the prompt!\n",
    "\n",
    "To do so, we will inherit from BaseChatModel and we’ll need to implement the following methods/properties:\n",
    "\n",
    "In addition, you have the option to specify the following:\n",
    "\n",
    "To do so inherit from BaseChatModel which is a lower level class and implement the methods:\n",
    "\n",
    "- _generate - Use to generate a chat result from a prompt\n",
    "\n",
    "- The property _llm_type - Used to uniquely identify the type of the model. Used for logging.\n",
    "\n",
    "Optional:\n",
    "\n",
    "- `_stream` - Use to implement streaming.\n",
    "\n",
    "- `_agenerate` - Use to implement a native async method.\n",
    "\n",
    "- `_astream` - Use to implement async version of `_stream`.\n",
    "\n",
    "- The property `_identifying_params` - Represent model parameterization for logging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "from langchain_core.pydantic_v1 import Field, root_validator\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "\n",
    "class FastChat(BaseChatModel):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = FastChat(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    model_name: str = Field(default=\"lmsys/fastchat-t5-3b-v1.0\", alias=\"model\")\n",
    "    \"\"\"Model name to use\"\"\"\n",
    "    \n",
    "    @root_validator\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        print(\"validate_environment\")\n",
    "\n",
    "        # Initialize a model. Not for sure this is the best place to load the model locally.\n",
    "        if not values.get(\"client\"):\n",
    "            print(\"Initialize local model\")\n",
    "            model_name = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "            values[\"client\"] = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        return values\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        print(\"The _generate function\", self.client)\n",
    "\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[: self.n]\n",
    "        message = AIMessage(content=tokens)\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Stream the output of the model.\n",
    "\n",
    "        This method should be implemented if the model can generate output\n",
    "        in a streaming fashion. If the model does not support streaming,\n",
    "        do not implement it. In that case streaming requests will be automatically\n",
    "        handled by the _generate method.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        print(\"The _stream function\")\n",
    "\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[: self.n]\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n",
    "\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[ChatGenerationChunk]:\n",
    "        \"\"\"An async variant of astream.\n",
    "\n",
    "        If not provided, the default behavior is to delegate to the _generate method.\n",
    "\n",
    "        The implementation below instead will delegate to `_stream` and will\n",
    "        kick it off in a separate thread.\n",
    "\n",
    "        If you're able to natively support async, then by all means do so!\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"The _astream function\")\n",
    "    \n",
    "        result = await run_in_executor(\n",
    "            None,\n",
    "            self._stream,\n",
    "            messages,\n",
    "            stop=stop,\n",
    "            run_manager=run_manager.get_sync() if run_manager else None,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for chunk in result:\n",
    "            yield chunk\n",
    "\n",
    "    def _load_fastchat_model(self):\n",
    "        \"\"\"Load the fastchat model locally\"\"\"\n",
    "        return AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_environment\n",
      "Initialize local model\n"
     ]
    }
   ],
   "source": [
    "model = FastChat(n=3)\n",
    "# model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "model_name = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "model_cache = \"~/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "message = \"Hello, are you a chat bot?\"\n",
    "\n",
    "inputs = tokenizer(message, return_tensors=\"pt\")\n",
    "output = model.generate(inputs[\"input_ids\"], max_new_tokens=100)[0]\n",
    "response = tokenizer.decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes,   I'm   a chatbot.   How   can   I   assist   you   today? \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
